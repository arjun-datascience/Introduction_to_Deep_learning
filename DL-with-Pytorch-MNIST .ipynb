{"cells":[{"metadata":{"colab":{},"colab_type":"code","id":"_17_ZnVDhCd0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch import nn,optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd","execution_count":1,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"colab_type":"code","id":"lb2lrOr7hbdx","outputId":"92158888-fb2a-491e-8462-0698f118e14e","trusted":true},"cell_type":"code","source":"\ntransform=transforms.Compose([transforms.ToTensor(),\n                             transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\ntrainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\ntestset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n\ntrainloader=torch.utils.data.DataLoader(trainset,batch_size=90,shuffle=True,num_workers=0)\n\ntestloader=torch.utils.data.DataLoader(testset,batch_size=90,shuffle=True,num_workers=0)\n","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\n","name":"stdout"}]},{"metadata":{"colab":{},"colab_type":"code","id":"fD_LJ06Mh0re","trusted":true},"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 90)\n        self.fc2 = nn.Linear(90, 30)\n        self.fc3 = nn.Linear(30, 10)\n\n        # Dropout module with 0.2 drop probability\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        # make sure input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # Now with dropout\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n\n        # output so no dropout here\n        x = F.log_softmax(self.fc3(x), dim=1)\n\n        return x\n        \nmodel=Network()\noptimizer = optim.SGD(model.parameters(), lr=0.007, momentum=0.9, weight_decay=3.0e-4, nesterov = True)\ncriterion=nn.NLLLoss()","execution_count":3,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102},"colab_type":"code","id":"OY0PFehch0ye","outputId":"14b32fc1-32e2-4d66-da92-379c436b9699","trusted":true},"cell_type":"code","source":"## Train\n\nepochs=10\ntrain_losses,test_losses=[],[]\nfor e in range(epochs):\n    running_loss=0\n    for images,labels in trainloader:\n        optimizer.zero_grad()\n        log_ps=model(images)\n        loss=criterion(log_ps,labels)\n        loss.backward()\n        optimizer.step()\n        running_loss+=loss.item()\n        \n    else:\n        test_loss=0\n        accuracy=0\n        \n        with torch.no_grad():\n            model.eval()\n            for images,labels in testloader:\n                log_ps=model(images)\n                test_loss+=criterion(log_ps,labels)\n                ps=torch.exp(log_ps)\n                top_p,top_class=ps.topk(1,dim=1)\n                equals=top_class==labels.view(*top_class.shape)\n                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n        model.train()\n        train_losses.append(running_loss/len(trainloader))\n        test_losses.append(test_loss/len(testloader))\n\n        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\npytorch_total_params = sum(p.numel() for p in model.parameters())\nprint(\"Total no. of parameters : \",pytorch_total_params )        \n        ","execution_count":4,"outputs":[{"output_type":"stream","text":"Epoch: 1/10..  Training Loss: 0.749..  Test Loss: 0.280..  Test Accuracy: 0.914\nEpoch: 2/10..  Training Loss: 0.352..  Test Loss: 0.199..  Test Accuracy: 0.938\nEpoch: 3/10..  Training Loss: 0.278..  Test Loss: 0.166..  Test Accuracy: 0.950\nEpoch: 4/10..  Training Loss: 0.240..  Test Loss: 0.142..  Test Accuracy: 0.957\nEpoch: 5/10..  Training Loss: 0.213..  Test Loss: 0.131..  Test Accuracy: 0.961\nEpoch: 6/10..  Training Loss: 0.194..  Test Loss: 0.115..  Test Accuracy: 0.966\nEpoch: 7/10..  Training Loss: 0.184..  Test Loss: 0.114..  Test Accuracy: 0.967\nEpoch: 8/10..  Training Loss: 0.175..  Test Loss: 0.104..  Test Accuracy: 0.969\nEpoch: 9/10..  Training Loss: 0.166..  Test Loss: 0.099..  Test Accuracy: 0.971\nEpoch: 10/10..  Training Loss: 0.161..  Test Loss: 0.103..  Test Accuracy: 0.970\nTotal no. of parameters :  73690\n","name":"stdout"}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"MNIST_assignment.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}